{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_of_instances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-26e596d437de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m#------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m#transfer train and test set data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_of_instances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0memotion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_of_instances' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#------------------------------\n",
    "#cpu - gpu configuration\n",
    "config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': 56} ) #max: 1 gpu, 56 cpu\n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "#------------------------------\n",
    "#variables\n",
    "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "#------------------------------\n",
    "#Controller parameter\n",
    "\n",
    "fit = False\n",
    "read_data = False\n",
    "#------------------------------\n",
    "#read kaggle facial expression recognition challenge dataset (fer2013.csv)\n",
    "#https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\n",
    "\n",
    "\n",
    "\n",
    "if read_data:\n",
    "    with open(\"data/fer2013/fer2013.csv\") as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    lines = np.array(content)\n",
    "\n",
    "    num_of_instances = lines.size\n",
    "    print(\"number of instances: \",num_of_instances)\n",
    "    print(\"instance length: \",len(lines[1].split(\",\")[1].split(\" \")))\n",
    "\n",
    "#------------------------------\n",
    "#initialize trainset and test set\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "#------------------------------\n",
    "#transfer train and test set data\n",
    "for i in range(1,num_of_instances):\n",
    "    try:\n",
    "        emotion, img, usage = lines[i].split(\",\")\n",
    "\n",
    "        val = img.split(\" \")\n",
    "\n",
    "        pixels = np.array(val, 'float32')\n",
    "\n",
    "        emotion = keras.utils.to_categorical(emotion, num_classes)\n",
    "\n",
    "        if 'Training' in usage:\n",
    "            y_train.append(emotion)\n",
    "            x_train.append(pixels)\n",
    "        elif 'PublicTest' in usage:\n",
    "            y_test.append(emotion)\n",
    "            x_test.append(pixels)\n",
    "    except:\n",
    "        print(\"\",end=\"\")\n",
    "\n",
    "#------------------------------\n",
    "#data transformation for train and test sets\n",
    "x_train = np.array(x_train, 'float32')\n",
    "y_train = np.array(y_train, 'float32')\n",
    "x_test = np.array(x_test, 'float32')\n",
    "y_test = np.array(y_test, 'float32')\n",
    "\n",
    "x_train /= 255 #normalize inputs between [0, 1]\n",
    "x_test /= 255\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "#------------------------------\n",
    "#construct CNN structure\n",
    "model = Sequential()\n",
    "\n",
    "#1st convolution layer\n",
    "model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n",
    "\n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "#3rd convolution layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#fully connected neural networks\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "#------------------------------\n",
    "#batch process\n",
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "model.compile(loss='categorical_crossentropy'\n",
    "    , optimizer=keras.optimizers.Adam()\n",
    "    , metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "if fit == True:\n",
    "    #model.fit_generator(x_train, y_train, epochs=epochs) #train for all trainset\n",
    "    model.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs) #train for randomly selected one\n",
    "else:\n",
    "    model.load_weights('data/facial_expression_model_weights.h5') #load weights\n",
    "\n",
    "#------------------------------\n",
    "\"\"\"\n",
    "#overall evaluation\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', 100*score[1])\n",
    "\"\"\"\n",
    "#------------------------------\n",
    "#function for drawing bar chart for emotion preditions\n",
    "def emotion_analysis(emotions):\n",
    "    plt.clf()\n",
    "    objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    y_pos = np.arange(len(objects))\n",
    "\n",
    "    plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('percentage')\n",
    "    plt.title('emotion')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def emotion_analysis_realtime(emotions):     \n",
    "    print(emotions)\n",
    "#     line1, = ax.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "#     plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "    line1.set_ydata(emotion)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "#------------------------------\n",
    "\n",
    "monitor_testset_results = False\n",
    "\n",
    "if monitor_testset_results == True:\n",
    "    #make predictions for test set\n",
    "    predictions = model.predict(x_test)\n",
    "\n",
    "    index = 0\n",
    "    for i in predictions:\n",
    "        if index < 30 and index >= 20:\n",
    "            #print(i) #predicted scores\n",
    "            #print(y_test[index]) #actual scores\n",
    "\n",
    "            testing_img = np.array(x_test[index], 'float32')\n",
    "            testing_img = testing_img.reshape([48, 48]);\n",
    "\n",
    "            plt.gray()\n",
    "            plt.imshow(testing_img)\n",
    "            plt.show()\n",
    "\n",
    "            print(i)\n",
    "\n",
    "            emotion_analysis(i)\n",
    "            print(\"----------------------------------------------\")\n",
    "        index = index + 1\n",
    "\n",
    "#------------------------------\n",
    "#make prediction for custom image out of test set\n",
    "img = image.load_img(\"/home/roboi9/workspace/Keras/Emotion Detection/data/test/ammar.jpg\",\n",
    "                     grayscale=True, target_size=(48, 48))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis = 0)\n",
    "\n",
    "x /= 255\n",
    "print(\"x size : \",x.shape)\n",
    "# custom = model.predict(x)\n",
    "# emotion_analysis(custom[0])\n",
    "\n",
    "# x = np.array(x, 'float32')\n",
    "# x = x.reshape([48, 48]);\n",
    "\n",
    "# plt.gray()\n",
    "# plt.imshow(x)\n",
    "# plt.show()\n",
    "# #------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "args = dict()\n",
    "args['detector'] = 'face_detection_model'\n",
    "args['embedding_model'] = 'openface_nn4.small2.v1.t7'\n",
    "args['confidence'] = 0.5\n",
    "protoPath = os.path.sep.join([args[\"detector\"], \"deploy.prototxt\"])\n",
    "modelPath = os.path.sep.join([args[\"detector\"],\n",
    "                              \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "embedder = cv2.dnn.readNetFromTorch(args[\"embedding_model\"])\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "fps = FPS().start()\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream\n",
    "    frame = vs.read() \n",
    "\n",
    "    # resize the frame to have a width of 600 pixels (while\n",
    "    # maintaining the aspect ratio), and then grab the image\n",
    "    # dimensions\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections\n",
    "        if confidence > args[\"confidence\"]:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # extract the face ROI\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "\n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "                \n",
    "           \n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "#             faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "#                 (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "#             embedder.setInput(faceBlob)\n",
    "#             vec = embedder.forward()\n",
    "\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            x = face\n",
    "            x = cv2.resize(x, dsize=(48, 48), interpolation=cv2.INTER_CUBIC)\n",
    "            x = np.expand_dims(x, axis = 0)\n",
    "            x = np.expand_dims(x, axis = 4)\n",
    "            custom = model.predict(x)\n",
    "            emotion_analysis(custom[0])\n",
    "\n",
    "#             x = np.array(x, 'float32')\n",
    "#             x = x.reshape([48, 48]);\n",
    "\n",
    "#             plt.gray()\n",
    "#             plt.imshow(x)\n",
    "#             plt.show()\n",
    "\n",
    "#             # perform classification to recognize the face\n",
    "#             preds = recognizer.predict_proba(vec)[0]\n",
    "#             j = np.argmax(preds)\n",
    "#             proba = preds[j]\n",
    "#             name = le.classes_[j]\n",
    "\n",
    "            # draw the bounding box of the face along with the\n",
    "            # associated probability\n",
    "#             text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "#             cv2.putText(frame, text, (startX, y),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "    # update the FPS counter\n",
    "    fps.update()\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
